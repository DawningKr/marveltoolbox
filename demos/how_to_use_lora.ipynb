{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd247d4",
   "metadata": {},
   "source": [
    "# Step 1: Define trainer without LoRA to train the Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24462aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf\n",
      "Batch size: 128\n",
      "Epochs:     50\n",
      "device:     cuda:0\n",
      "\n",
      "image range [0, 1]\n",
      "Set random seed to: 0\n",
      "Log file save at:  ./logs/demo-mnist-clf.log\n",
      "Epoch/Iter:000/0000 Train Loss:3.006286 \n",
      "Epoch/Iter:000/0100 Train Loss:0.508709 \n",
      "Epoch/Iter:000/0200 Train Loss:0.279684 \n",
      "Epoch/Iter:000/0300 Train Loss:0.196592 \n",
      "acc: 0.9804166666666667\n",
      "Epoch: 001/050 2% [Remain: 0h/ 1m/58s | Avg: 2.13s/Epoch]\n",
      "\n",
      "Epoch/Iter:001/0000 Train Loss:0.120898 \n",
      "Epoch/Iter:001/0100 Train Loss:0.078509 \n",
      "Epoch/Iter:001/0200 Train Loss:0.084688 \n",
      "Epoch/Iter:001/0300 Train Loss:0.083621 \n",
      "acc: 0.9838333333333333\n",
      "Epoch: 002/050 4% [Remain: 0h/ 2m/36s | Avg: 2.08s/Epoch]\n",
      "\n",
      "Epoch/Iter:002/0000 Train Loss:0.054317 \n",
      "Epoch/Iter:002/0100 Train Loss:0.033758 \n",
      "Epoch/Iter:002/0200 Train Loss:0.066338 \n",
      "Epoch/Iter:002/0300 Train Loss:0.052845 \n",
      "acc: 0.9825\n",
      "Epoch: 003/050 6% [Remain: 0h/ 2m/47s | Avg: 2.06s/Epoch]\n",
      "\n",
      "Epoch/Iter:003/0000 Train Loss:0.040292 \n",
      "Epoch/Iter:003/0100 Train Loss:0.036202 \n",
      "Epoch/Iter:003/0200 Train Loss:0.031174 \n",
      "Epoch/Iter:003/0300 Train Loss:0.045130 \n",
      "acc: 0.9835833333333334\n",
      "Epoch: 004/050 8% [Remain: 0h/ 2m/40s | Avg: 2.05s/Epoch]\n",
      "\n",
      "Epoch/Iter:004/0000 Train Loss:0.032690 \n",
      "Epoch/Iter:004/0100 Train Loss:0.007621 \n",
      "Epoch/Iter:004/0200 Train Loss:0.014620 \n",
      "Epoch/Iter:004/0300 Train Loss:0.012918 \n",
      "acc: 0.98625\n",
      "Epoch: 005/050 10% [Remain: 0h/ 2m/34s | Avg: 2.05s/Epoch]\n",
      "\n",
      "Epoch/Iter:005/0000 Train Loss:0.027816 \n",
      "Epoch/Iter:005/0100 Train Loss:0.010192 \n",
      "Epoch/Iter:005/0200 Train Loss:0.011501 \n",
      "Epoch/Iter:005/0300 Train Loss:0.031281 \n",
      "acc: 0.9851666666666666\n",
      "Epoch: 006/050 12% [Remain: 0h/ 2m/34s | Avg: 2.04s/Epoch]\n",
      "\n",
      "Epoch/Iter:006/0000 Train Loss:0.015497 \n",
      "Epoch/Iter:006/0100 Train Loss:0.004635 \n",
      "Epoch/Iter:006/0200 Train Loss:0.044518 \n",
      "Epoch/Iter:006/0300 Train Loss:0.012754 \n",
      "acc: 0.9893333333333333\n",
      "Epoch: 007/050 14% [Remain: 0h/ 2m/29s | Avg: 2.04s/Epoch]\n",
      "\n",
      "Epoch/Iter:007/0000 Train Loss:0.009927 \n",
      "Epoch/Iter:007/0100 Train Loss:0.021708 \n",
      "Epoch/Iter:007/0200 Train Loss:0.004693 \n",
      "Epoch/Iter:007/0300 Train Loss:0.012090 \n",
      "acc: 0.98925\n",
      "Epoch: 008/050 16% [Remain: 0h/ 2m/29s | Avg: 2.04s/Epoch]\n",
      "\n",
      "Epoch/Iter:008/0000 Train Loss:0.007504 \n",
      "Epoch/Iter:008/0100 Train Loss:0.007087 \n",
      "Epoch/Iter:008/0200 Train Loss:0.001965 \n",
      "Epoch/Iter:008/0300 Train Loss:0.032110 \n",
      "acc: 0.9900833333333333\n",
      "Epoch: 009/050 18% [Remain: 0h/ 2m/24s | Avg: 2.04s/Epoch]\n",
      "\n",
      "Epoch/Iter:009/0000 Train Loss:0.007179 \n",
      "Epoch/Iter:009/0100 Train Loss:0.006440 \n",
      "Epoch/Iter:009/0200 Train Loss:0.027965 \n",
      "Epoch/Iter:009/0300 Train Loss:0.002904 \n",
      "acc: 0.9863333333333333\n",
      "Epoch: 010/050 20% [Remain: 0h/ 2m/23s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:010/0000 Train Loss:0.004804 \n",
      "Epoch/Iter:010/0100 Train Loss:0.005483 \n",
      "Epoch/Iter:010/0200 Train Loss:0.001424 \n",
      "Epoch/Iter:010/0300 Train Loss:0.007178 \n",
      "acc: 0.9881666666666666\n",
      "Epoch: 011/050 22% [Remain: 0h/ 2m/19s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:011/0000 Train Loss:0.018672 \n",
      "Epoch/Iter:011/0100 Train Loss:0.001274 \n",
      "Epoch/Iter:011/0200 Train Loss:0.014967 \n",
      "Epoch/Iter:011/0300 Train Loss:0.006854 \n",
      "acc: 0.99075\n",
      "Epoch: 012/050 24% [Remain: 0h/ 2m/14s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:012/0000 Train Loss:0.002401 \n",
      "Epoch/Iter:012/0100 Train Loss:0.010354 \n",
      "Epoch/Iter:012/0200 Train Loss:0.008479 \n",
      "Epoch/Iter:012/0300 Train Loss:0.002823 \n",
      "acc: 0.9885833333333334\n",
      "Epoch: 013/050 26% [Remain: 0h/ 2m/12s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:013/0000 Train Loss:0.002497 \n",
      "Epoch/Iter:013/0100 Train Loss:0.002511 \n",
      "Epoch/Iter:013/0200 Train Loss:0.000618 \n",
      "Epoch/Iter:013/0300 Train Loss:0.009827 \n",
      "acc: 0.98925\n",
      "Epoch: 014/050 28% [Remain: 0h/ 2m/ 8s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:014/0000 Train Loss:0.002509 \n",
      "Epoch/Iter:014/0100 Train Loss:0.004921 \n",
      "Epoch/Iter:014/0200 Train Loss:0.001203 \n",
      "Epoch/Iter:014/0300 Train Loss:0.001783 \n",
      "acc: 0.9909166666666667\n",
      "Epoch: 015/050 30% [Remain: 0h/ 2m/ 4s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:015/0000 Train Loss:0.002903 \n",
      "Epoch/Iter:015/0100 Train Loss:0.001519 \n",
      "Epoch/Iter:015/0200 Train Loss:0.004983 \n",
      "Epoch/Iter:015/0300 Train Loss:0.001893 \n",
      "acc: 0.9884166666666667\n",
      "Epoch: 016/050 32% [Remain: 0h/ 2m/ 1s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:016/0000 Train Loss:0.003319 \n",
      "Epoch/Iter:016/0100 Train Loss:0.000814 \n",
      "Epoch/Iter:016/0200 Train Loss:0.004962 \n",
      "Epoch/Iter:016/0300 Train Loss:0.000548 \n",
      "acc: 0.9895\n",
      "Epoch: 017/050 34% [Remain: 0h/ 1m/57s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:017/0000 Train Loss:0.000961 \n",
      "Epoch/Iter:017/0100 Train Loss:0.000902 \n",
      "Epoch/Iter:017/0200 Train Loss:0.002465 \n",
      "Epoch/Iter:017/0300 Train Loss:0.000703 \n",
      "acc: 0.9875\n",
      "Epoch: 018/050 36% [Remain: 0h/ 1m/53s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:018/0000 Train Loss:0.000953 \n",
      "Epoch/Iter:018/0100 Train Loss:0.003690 \n",
      "Epoch/Iter:018/0200 Train Loss:0.001171 \n",
      "Epoch/Iter:018/0300 Train Loss:0.001145 \n",
      "acc: 0.989\n",
      "Epoch: 019/050 38% [Remain: 0h/ 1m/49s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:019/0000 Train Loss:0.003001 \n",
      "Epoch/Iter:019/0100 Train Loss:0.000225 \n",
      "Epoch/Iter:019/0200 Train Loss:0.000318 \n",
      "Epoch/Iter:019/0300 Train Loss:0.004084 \n",
      "acc: 0.99025\n",
      "Epoch: 020/050 40% [Remain: 0h/ 1m/45s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:020/0000 Train Loss:0.011164 \n",
      "Epoch/Iter:020/0100 Train Loss:0.005060 \n",
      "Epoch/Iter:020/0200 Train Loss:0.000391 \n",
      "Epoch/Iter:020/0300 Train Loss:0.003815 \n",
      "acc: 0.9893333333333333\n",
      "Epoch: 021/050 42% [Remain: 0h/ 1m/41s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:021/0000 Train Loss:0.002685 \n",
      "Epoch/Iter:021/0100 Train Loss:0.000746 \n",
      "Epoch/Iter:021/0200 Train Loss:0.004024 \n",
      "Epoch/Iter:021/0300 Train Loss:0.005415 \n",
      "acc: 0.9928333333333333\n",
      "Epoch: 022/050 44% [Remain: 0h/ 1m/36s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:022/0000 Train Loss:0.000593 \n",
      "Epoch/Iter:022/0100 Train Loss:0.000978 \n",
      "Epoch/Iter:022/0200 Train Loss:0.000539 \n",
      "Epoch/Iter:022/0300 Train Loss:0.004752 \n",
      "acc: 0.9916666666666667\n",
      "Epoch: 023/050 46% [Remain: 0h/ 1m/34s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:023/0000 Train Loss:0.001114 \n",
      "Epoch/Iter:023/0100 Train Loss:0.001017 \n",
      "Epoch/Iter:023/0200 Train Loss:0.000647 \n",
      "Epoch/Iter:023/0300 Train Loss:0.001422 \n",
      "acc: 0.9926666666666667\n",
      "Epoch: 024/050 48% [Remain: 0h/ 1m/30s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:024/0000 Train Loss:0.000903 \n",
      "Epoch/Iter:024/0100 Train Loss:0.000533 \n",
      "Epoch/Iter:024/0200 Train Loss:0.000329 \n",
      "Epoch/Iter:024/0300 Train Loss:0.000832 \n",
      "acc: 0.9918333333333333\n",
      "Epoch: 025/050 50% [Remain: 0h/ 1m/26s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:025/0000 Train Loss:0.000687 \n",
      "Epoch/Iter:025/0100 Train Loss:0.000655 \n",
      "Epoch/Iter:025/0200 Train Loss:0.001034 \n",
      "Epoch/Iter:025/0300 Train Loss:0.000905 \n",
      "acc: 0.99\n",
      "Epoch: 026/050 52% [Remain: 0h/ 1m/23s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:026/0000 Train Loss:0.000502 \n",
      "Epoch/Iter:026/0100 Train Loss:0.000131 \n",
      "Epoch/Iter:026/0200 Train Loss:0.000208 \n",
      "Epoch/Iter:026/0300 Train Loss:0.000400 \n",
      "acc: 0.9906666666666667\n",
      "Epoch: 027/050 54% [Remain: 0h/ 1m/19s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:027/0000 Train Loss:0.001071 \n",
      "Epoch/Iter:027/0100 Train Loss:0.001494 \n",
      "Epoch/Iter:027/0200 Train Loss:0.000218 \n",
      "Epoch/Iter:027/0300 Train Loss:0.000902 \n",
      "acc: 0.991\n",
      "Epoch: 028/050 56% [Remain: 0h/ 1m/16s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:028/0000 Train Loss:0.000431 \n",
      "Epoch/Iter:028/0100 Train Loss:0.007669 \n",
      "Epoch/Iter:028/0200 Train Loss:0.000558 \n",
      "Epoch/Iter:028/0300 Train Loss:0.000168 \n",
      "acc: 0.9904166666666666\n",
      "Epoch: 029/050 58% [Remain: 0h/ 1m/12s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:029/0000 Train Loss:0.000974 \n",
      "Epoch/Iter:029/0100 Train Loss:0.000116 \n",
      "Epoch/Iter:029/0200 Train Loss:0.000505 \n",
      "Epoch/Iter:029/0300 Train Loss:0.000474 \n",
      "acc: 0.9905833333333334\n",
      "Epoch: 030/050 60% [Remain: 0h/ 1m/ 8s | Avg: 2.03s/Epoch]\n",
      "\n",
      "Epoch/Iter:030/0000 Train Loss:0.005184 \n",
      "Epoch/Iter:030/0100 Train Loss:0.000253 \n",
      "Epoch/Iter:030/0200 Train Loss:0.000149 \n",
      "Epoch/Iter:030/0300 Train Loss:0.000383 \n",
      "acc: 0.9919166666666667\n",
      "Epoch: 031/050 62% [Remain: 0h/ 1m/ 5s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:031/0000 Train Loss:0.000732 \n",
      "Epoch/Iter:031/0100 Train Loss:0.000279 \n",
      "Epoch/Iter:031/0200 Train Loss:0.004587 \n",
      "Epoch/Iter:031/0300 Train Loss:0.000617 \n",
      "acc: 0.9904166666666666\n",
      "Epoch: 032/050 64% [Remain: 0h/ 1m/ 1s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:032/0000 Train Loss:0.000248 \n",
      "Epoch/Iter:032/0100 Train Loss:0.000088 \n",
      "Epoch/Iter:032/0200 Train Loss:0.005763 \n",
      "Epoch/Iter:032/0300 Train Loss:0.000168 \n",
      "acc: 0.9906666666666667\n",
      "Epoch: 033/050 66% [Remain: 0h/ 0m/58s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:033/0000 Train Loss:0.035710 \n",
      "Epoch/Iter:033/0100 Train Loss:0.018799 \n",
      "Epoch/Iter:033/0200 Train Loss:0.004366 \n",
      "Epoch/Iter:033/0300 Train Loss:0.000383 \n",
      "acc: 0.9923333333333333\n",
      "Epoch: 034/050 68% [Remain: 0h/ 0m/54s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:034/0000 Train Loss:0.001519 \n",
      "Epoch/Iter:034/0100 Train Loss:0.000490 \n",
      "Epoch/Iter:034/0200 Train Loss:0.001848 \n",
      "Epoch/Iter:034/0300 Train Loss:0.000632 \n",
      "acc: 0.9925\n",
      "Epoch: 035/050 70% [Remain: 0h/ 0m/51s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:035/0000 Train Loss:0.000073 \n",
      "Epoch/Iter:035/0100 Train Loss:0.000132 \n",
      "Epoch/Iter:035/0200 Train Loss:0.000162 \n",
      "Epoch/Iter:035/0300 Train Loss:0.068406 \n",
      "acc: 0.9925833333333334\n",
      "Epoch: 036/050 72% [Remain: 0h/ 0m/47s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:036/0000 Train Loss:0.001995 \n",
      "Epoch/Iter:036/0100 Train Loss:0.000029 \n",
      "Epoch/Iter:036/0200 Train Loss:0.000174 \n",
      "Epoch/Iter:036/0300 Train Loss:0.000238 \n",
      "acc: 0.9900833333333333\n",
      "Epoch: 037/050 74% [Remain: 0h/ 0m/44s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:037/0000 Train Loss:0.028221 \n",
      "Epoch/Iter:037/0100 Train Loss:0.000120 \n",
      "Epoch/Iter:037/0200 Train Loss:0.002763 \n",
      "Epoch/Iter:037/0300 Train Loss:0.000128 \n",
      "acc: 0.9925\n",
      "Epoch: 038/050 76% [Remain: 0h/ 0m/40s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:038/0000 Train Loss:0.000285 \n",
      "Epoch/Iter:038/0100 Train Loss:0.000059 \n",
      "Epoch/Iter:038/0200 Train Loss:0.000238 \n",
      "Epoch/Iter:038/0300 Train Loss:0.000883 \n",
      "acc: 0.9900833333333333\n",
      "Epoch: 039/050 78% [Remain: 0h/ 0m/37s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:039/0000 Train Loss:0.001543 \n",
      "Epoch/Iter:039/0100 Train Loss:0.000087 \n",
      "Epoch/Iter:039/0200 Train Loss:0.000199 \n",
      "Epoch/Iter:039/0300 Train Loss:0.005254 \n",
      "acc: 0.9908333333333333\n",
      "Epoch: 040/050 80% [Remain: 0h/ 0m/33s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:040/0000 Train Loss:0.000137 \n",
      "Epoch/Iter:040/0100 Train Loss:0.000992 \n",
      "Epoch/Iter:040/0200 Train Loss:0.020325 \n",
      "Epoch/Iter:040/0300 Train Loss:0.026394 \n",
      "acc: 0.9918333333333333\n",
      "Epoch: 041/050 82% [Remain: 0h/ 0m/30s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:041/0000 Train Loss:0.000515 \n",
      "Epoch/Iter:041/0100 Train Loss:0.000008 \n",
      "Epoch/Iter:041/0200 Train Loss:0.000740 \n",
      "Epoch/Iter:041/0300 Train Loss:0.000048 \n",
      "acc: 0.9914166666666666\n",
      "Epoch: 042/050 84% [Remain: 0h/ 0m/26s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:042/0000 Train Loss:0.000290 \n",
      "Epoch/Iter:042/0100 Train Loss:0.000044 \n",
      "Epoch/Iter:042/0200 Train Loss:0.000016 \n",
      "Epoch/Iter:042/0300 Train Loss:0.000205 \n",
      "acc: 0.9904166666666666\n",
      "Epoch: 043/050 86% [Remain: 0h/ 0m/23s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:043/0000 Train Loss:0.000592 \n",
      "Epoch/Iter:043/0100 Train Loss:0.000569 \n",
      "Epoch/Iter:043/0200 Train Loss:0.000645 \n",
      "Epoch/Iter:043/0300 Train Loss:0.000418 \n",
      "acc: 0.99125\n",
      "Epoch: 044/050 88% [Remain: 0h/ 0m/20s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:044/0000 Train Loss:0.002580 \n",
      "Epoch/Iter:044/0100 Train Loss:0.000172 \n",
      "Epoch/Iter:044/0200 Train Loss:0.001012 \n",
      "Epoch/Iter:044/0300 Train Loss:0.000246 \n",
      "acc: 0.9909166666666667\n",
      "Epoch: 045/050 90% [Remain: 0h/ 0m/16s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:045/0000 Train Loss:0.001056 \n",
      "Epoch/Iter:045/0100 Train Loss:0.000185 \n",
      "Epoch/Iter:045/0200 Train Loss:0.000039 \n",
      "Epoch/Iter:045/0300 Train Loss:0.000142 \n",
      "acc: 0.9919166666666667\n",
      "Epoch: 046/050 92% [Remain: 0h/ 0m/13s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:046/0000 Train Loss:0.002849 \n",
      "Epoch/Iter:046/0100 Train Loss:0.000045 \n",
      "Epoch/Iter:046/0200 Train Loss:0.000195 \n",
      "Epoch/Iter:046/0300 Train Loss:0.013629 \n",
      "acc: 0.99125\n",
      "Epoch: 047/050 94% [Remain: 0h/ 0m/10s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:047/0000 Train Loss:0.000216 \n",
      "Epoch/Iter:047/0100 Train Loss:0.000573 \n",
      "Epoch/Iter:047/0200 Train Loss:0.000048 \n",
      "Epoch/Iter:047/0300 Train Loss:0.000084 \n",
      "acc: 0.9910833333333333\n",
      "Epoch: 048/050 96% [Remain: 0h/ 0m/ 6s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:048/0000 Train Loss:0.000210 \n",
      "Epoch/Iter:048/0100 Train Loss:0.001975 \n",
      "Epoch/Iter:048/0200 Train Loss:0.000393 \n",
      "Epoch/Iter:048/0300 Train Loss:0.000243 \n",
      "acc: 0.9894166666666667\n",
      "Epoch: 049/050 98% [Remain: 0h/ 0m/ 3s | Avg: 2.02s/Epoch]\n",
      "\n",
      "Epoch/Iter:049/0000 Train Loss:0.000469 \n",
      "Epoch/Iter:049/0100 Train Loss:0.000147 \n",
      "Epoch/Iter:049/0200 Train Loss:0.001575 \n",
      "Epoch/Iter:049/0300 Train Loss:0.000348 \n",
      "acc: 0.9925\n",
      "Epoch: 050/050 100% [Remain: 0h/ 0m/ 0s | Avg: 2.02s/Epoch]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import marveltoolbox as mt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Confs(mt.BaseConfs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        self.dataset = 'mnist'\n",
    "        self.nc = 1\n",
    "        self.nz = 10\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 50\n",
    "        self.seed = 0\n",
    "\n",
    "    def get_flag(self):\n",
    "        self.flag = 'demo-{}-clf'.format(self.dataset)\n",
    "\n",
    "    def get_device(self):\n",
    "        self.device_ids = [0]\n",
    "        self.ngpu = len(self.device_ids)\n",
    "        self.device = torch.device(\n",
    "            \"cuda:{}\".format(self.device_ids[0]) if \\\n",
    "            (torch.cuda.is_available() and self.ngpu > 0) else \"mps\")\n",
    "\n",
    "\n",
    "class Trainer(mt.BaseTrainer, Confs):\n",
    "    def __init__(self):\n",
    "        Confs.__init__(self)\n",
    "        mt.BaseTrainer.__init__(self, self)\n",
    "        \n",
    "        self.models['C'] = mt.nn.dcgan.Enet32(self.nc, self.nz).to(self.device) \n",
    "\n",
    "        self.optims['C'] = torch.optim.Adam(\n",
    "            self.models['C'].parameters(), lr=1e-4, betas=(0.5, 0.99))\n",
    "        \n",
    "        self.train_loader, self.val_loader, self.test_loader, _ = \\\n",
    "            mt.datasets.load_data(self.dataset, 1.0, 0.8, self.batch_size, 32, None, False)\n",
    "\n",
    "        self.records['acc'] = 0.0\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.models['C'].train()\n",
    "        for i, (x, y) in enumerate(self.train_loader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            scores = self.models['C'](x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            self.optims['C'].zero_grad()\n",
    "            loss.backward()\n",
    "            self.optims['C'].step()\n",
    "            if i % 100 == 0:\n",
    "                self.logs['Train Loss'] = loss.item()\n",
    "                self.print_logs(epoch, i)\n",
    "\n",
    "        return loss.item()\n",
    "                \n",
    "    def eval(self, epoch):\n",
    "        self.models['C'].eval()\n",
    "        correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                N = len(x)\n",
    "                scores = self.models['C'](x)\n",
    "                pred_y = torch.argmax(scores, dim=1)\n",
    "                correct += torch.sum(pred_y == y).item()\n",
    "        N = len(self.val_loader.dataset)\n",
    "        acc = correct / N\n",
    "        is_best = False\n",
    "        if acc >= self.records['acc']:\n",
    "            is_best = True\n",
    "            self.records['acc'] = acc\n",
    "        print('acc: {}'.format(acc))\n",
    "        return is_best\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer_base = Trainer()\n",
    "    trainer_base.run(load_best=False, retrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3180477",
   "metadata": {},
   "source": [
    "# Step2: Define a LoRA trainer that takes a base model as input and injects LoRA weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13fd809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf-lora-r4-alpha1.0\n",
      "Batch size: 128\n",
      "Epochs:     50\n",
      "device:     cuda:0\n",
      "\n",
      "image range [0, 1]\n",
      "Set random seed to: 0\n",
      "Log file save at:  ./logs/demo-mnist-clf-lora-r4-alpha1.0.log\n",
      "Epoch/Iter:000/0000 Train Loss:0.000200 \n",
      "Epoch/Iter:000/0100 Train Loss:0.001602 \n",
      "Epoch/Iter:000/0200 Train Loss:0.000417 \n",
      "Epoch/Iter:000/0300 Train Loss:0.062222 \n",
      "acc: 0.99875\n",
      "Epoch: 001/050 2% [Remain: 0h/ 1m/49s | Avg: 1.93s/Epoch]\n",
      "\n",
      "Epoch/Iter:001/0000 Train Loss:0.000089 \n",
      "Epoch/Iter:001/0100 Train Loss:0.002881 \n",
      "Epoch/Iter:001/0200 Train Loss:0.016062 \n",
      "Epoch/Iter:001/0300 Train Loss:0.000059 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 002/050 4% [Remain: 0h/ 1m/47s | Avg: 1.92s/Epoch]\n",
      "\n",
      "Epoch/Iter:002/0000 Train Loss:0.007505 \n",
      "Epoch/Iter:002/0100 Train Loss:0.000097 \n",
      "Epoch/Iter:002/0200 Train Loss:0.006681 \n",
      "Epoch/Iter:002/0300 Train Loss:0.000346 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 003/050 6% [Remain: 0h/ 1m/49s | Avg: 1.92s/Epoch]\n",
      "\n",
      "Epoch/Iter:003/0000 Train Loss:0.000103 \n",
      "Epoch/Iter:003/0100 Train Loss:0.011148 \n",
      "Epoch/Iter:003/0200 Train Loss:0.000105 \n",
      "Epoch/Iter:003/0300 Train Loss:0.001128 \n",
      "acc: 0.99875\n",
      "Epoch: 004/050 8% [Remain: 0h/ 1m/52s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:004/0000 Train Loss:0.001750 \n",
      "Epoch/Iter:004/0100 Train Loss:0.000112 \n",
      "Epoch/Iter:004/0200 Train Loss:0.000124 \n",
      "Epoch/Iter:004/0300 Train Loss:0.002306 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 005/050 10% [Remain: 0h/ 1m/50s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:005/0000 Train Loss:0.005040 \n",
      "Epoch/Iter:005/0100 Train Loss:0.000017 \n",
      "Epoch/Iter:005/0200 Train Loss:0.000101 \n",
      "Epoch/Iter:005/0300 Train Loss:0.104579 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 006/050 12% [Remain: 0h/ 1m/48s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:006/0000 Train Loss:0.000022 \n",
      "Epoch/Iter:006/0100 Train Loss:0.000026 \n",
      "Epoch/Iter:006/0200 Train Loss:0.017639 \n",
      "Epoch/Iter:006/0300 Train Loss:0.005364 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 007/050 14% [Remain: 0h/ 1m/48s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:007/0000 Train Loss:0.000049 \n",
      "Epoch/Iter:007/0100 Train Loss:0.025305 \n",
      "Epoch/Iter:007/0200 Train Loss:0.001047 \n",
      "Epoch/Iter:007/0300 Train Loss:0.000136 \n",
      "acc: 0.99875\n",
      "Epoch: 008/050 16% [Remain: 0h/ 1m/47s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:008/0000 Train Loss:0.000422 \n",
      "Epoch/Iter:008/0100 Train Loss:0.084202 \n",
      "Epoch/Iter:008/0200 Train Loss:0.000131 \n",
      "Epoch/Iter:008/0300 Train Loss:0.000033 \n",
      "acc: 0.99875\n",
      "Epoch: 009/050 18% [Remain: 0h/ 1m/44s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:009/0000 Train Loss:0.023180 \n",
      "Epoch/Iter:009/0100 Train Loss:0.008899 \n",
      "Epoch/Iter:009/0200 Train Loss:0.002539 \n",
      "Epoch/Iter:009/0300 Train Loss:0.084502 \n",
      "acc: 0.9989166666666667\n",
      "Epoch: 010/050 20% [Remain: 0h/ 1m/41s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:010/0000 Train Loss:0.000189 \n",
      "Epoch/Iter:010/0100 Train Loss:0.000154 \n",
      "Epoch/Iter:010/0200 Train Loss:0.000162 \n",
      "Epoch/Iter:010/0300 Train Loss:0.000059 \n",
      "acc: 0.9989166666666667\n",
      "Epoch: 011/050 22% [Remain: 0h/ 1m/39s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:011/0000 Train Loss:0.000240 \n",
      "Epoch/Iter:011/0100 Train Loss:0.000376 \n",
      "Epoch/Iter:011/0200 Train Loss:0.000219 \n",
      "Epoch/Iter:011/0300 Train Loss:0.001675 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 012/050 24% [Remain: 0h/ 1m/38s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:012/0000 Train Loss:0.004202 \n",
      "Epoch/Iter:012/0100 Train Loss:0.000179 \n",
      "Epoch/Iter:012/0200 Train Loss:0.000721 \n",
      "Epoch/Iter:012/0300 Train Loss:0.001939 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 013/050 26% [Remain: 0h/ 1m/35s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:013/0000 Train Loss:0.000108 \n",
      "Epoch/Iter:013/0100 Train Loss:0.005536 \n",
      "Epoch/Iter:013/0200 Train Loss:0.000054 \n",
      "Epoch/Iter:013/0300 Train Loss:0.000314 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 014/050 28% [Remain: 0h/ 1m/32s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:014/0000 Train Loss:0.003542 \n",
      "Epoch/Iter:014/0100 Train Loss:0.001177 \n",
      "Epoch/Iter:014/0200 Train Loss:0.021110 \n",
      "Epoch/Iter:014/0300 Train Loss:0.000068 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 015/050 30% [Remain: 0h/ 1m/29s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:015/0000 Train Loss:0.000208 \n",
      "Epoch/Iter:015/0100 Train Loss:0.000322 \n",
      "Epoch/Iter:015/0200 Train Loss:0.000390 \n",
      "Epoch/Iter:015/0300 Train Loss:0.000035 \n",
      "acc: 0.99875\n",
      "Epoch: 016/050 32% [Remain: 0h/ 1m/27s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:016/0000 Train Loss:0.000064 \n",
      "Epoch/Iter:016/0100 Train Loss:0.000095 \n",
      "Epoch/Iter:016/0200 Train Loss:0.001337 \n",
      "Epoch/Iter:016/0300 Train Loss:0.000284 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 017/050 34% [Remain: 0h/ 1m/24s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:017/0000 Train Loss:0.000226 \n",
      "Epoch/Iter:017/0100 Train Loss:0.000605 \n",
      "Epoch/Iter:017/0200 Train Loss:0.000107 \n",
      "Epoch/Iter:017/0300 Train Loss:0.000143 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 018/050 36% [Remain: 0h/ 1m/21s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:018/0000 Train Loss:0.000136 \n",
      "Epoch/Iter:018/0100 Train Loss:0.000293 \n",
      "Epoch/Iter:018/0200 Train Loss:0.000143 \n",
      "Epoch/Iter:018/0300 Train Loss:0.000291 \n",
      "acc: 0.99875\n",
      "Epoch: 019/050 38% [Remain: 0h/ 1m/19s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:019/0000 Train Loss:0.000779 \n",
      "Epoch/Iter:019/0100 Train Loss:0.006716 \n",
      "Epoch/Iter:019/0200 Train Loss:0.001151 \n",
      "Epoch/Iter:019/0300 Train Loss:0.000461 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 020/050 40% [Remain: 0h/ 1m/16s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:020/0000 Train Loss:0.000310 \n",
      "Epoch/Iter:020/0100 Train Loss:0.000118 \n",
      "Epoch/Iter:020/0200 Train Loss:0.000066 \n",
      "Epoch/Iter:020/0300 Train Loss:0.000221 \n",
      "acc: 0.99875\n",
      "Epoch: 021/050 42% [Remain: 0h/ 1m/14s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:021/0000 Train Loss:0.000075 \n",
      "Epoch/Iter:021/0100 Train Loss:0.000088 \n",
      "Epoch/Iter:021/0200 Train Loss:0.000138 \n",
      "Epoch/Iter:021/0300 Train Loss:0.000116 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 022/050 44% [Remain: 0h/ 1m/11s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:022/0000 Train Loss:0.000120 \n",
      "Epoch/Iter:022/0100 Train Loss:0.001461 \n",
      "Epoch/Iter:022/0200 Train Loss:0.000415 \n",
      "Epoch/Iter:022/0300 Train Loss:0.007220 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 023/050 46% [Remain: 0h/ 1m/ 8s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:023/0000 Train Loss:0.000252 \n",
      "Epoch/Iter:023/0100 Train Loss:0.000066 \n",
      "Epoch/Iter:023/0200 Train Loss:0.000410 \n",
      "Epoch/Iter:023/0300 Train Loss:0.000142 \n",
      "acc: 0.99875\n",
      "Epoch: 024/050 48% [Remain: 0h/ 1m/ 6s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:024/0000 Train Loss:0.001894 \n",
      "Epoch/Iter:024/0100 Train Loss:0.000070 \n",
      "Epoch/Iter:024/0200 Train Loss:0.000165 \n",
      "Epoch/Iter:024/0300 Train Loss:0.000586 \n",
      "acc: 0.99875\n",
      "Epoch: 025/050 50% [Remain: 0h/ 1m/ 3s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:025/0000 Train Loss:0.088202 \n",
      "Epoch/Iter:025/0100 Train Loss:0.000272 \n",
      "Epoch/Iter:025/0200 Train Loss:0.001355 \n",
      "Epoch/Iter:025/0300 Train Loss:0.000115 \n",
      "acc: 0.99875\n",
      "Epoch: 026/050 52% [Remain: 0h/ 1m/ 1s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:026/0000 Train Loss:0.000549 \n",
      "Epoch/Iter:026/0100 Train Loss:0.000160 \n",
      "Epoch/Iter:026/0200 Train Loss:0.000983 \n",
      "Epoch/Iter:026/0300 Train Loss:0.000232 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 027/050 54% [Remain: 0h/ 0m/58s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:027/0000 Train Loss:0.001863 \n",
      "Epoch/Iter:027/0100 Train Loss:0.000125 \n",
      "Epoch/Iter:027/0200 Train Loss:0.000046 \n",
      "Epoch/Iter:027/0300 Train Loss:0.000344 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 028/050 56% [Remain: 0h/ 0m/55s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:028/0000 Train Loss:0.000135 \n",
      "Epoch/Iter:028/0100 Train Loss:0.000094 \n",
      "Epoch/Iter:028/0200 Train Loss:0.001342 \n",
      "Epoch/Iter:028/0300 Train Loss:0.004902 \n",
      "acc: 0.99875\n",
      "Epoch: 029/050 58% [Remain: 0h/ 0m/53s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:029/0000 Train Loss:0.000226 \n",
      "Epoch/Iter:029/0100 Train Loss:0.000212 \n",
      "Epoch/Iter:029/0200 Train Loss:0.000198 \n",
      "Epoch/Iter:029/0300 Train Loss:0.000310 \n",
      "acc: 0.99875\n",
      "Epoch: 030/050 60% [Remain: 0h/ 0m/50s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:030/0000 Train Loss:0.000142 \n",
      "Epoch/Iter:030/0100 Train Loss:0.000826 \n",
      "Epoch/Iter:030/0200 Train Loss:0.000436 \n",
      "Epoch/Iter:030/0300 Train Loss:0.003529 \n",
      "acc: 0.99875\n",
      "Epoch: 031/050 62% [Remain: 0h/ 0m/48s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:031/0000 Train Loss:0.000144 \n",
      "Epoch/Iter:031/0100 Train Loss:0.000060 \n",
      "Epoch/Iter:031/0200 Train Loss:0.000680 \n",
      "Epoch/Iter:031/0300 Train Loss:0.000594 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 032/050 64% [Remain: 0h/ 0m/45s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:032/0000 Train Loss:0.000061 \n",
      "Epoch/Iter:032/0100 Train Loss:0.000274 \n",
      "Epoch/Iter:032/0200 Train Loss:0.000133 \n",
      "Epoch/Iter:032/0300 Train Loss:0.000220 \n",
      "acc: 0.9989166666666667\n",
      "Epoch: 033/050 66% [Remain: 0h/ 0m/43s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:033/0000 Train Loss:0.001038 \n",
      "Epoch/Iter:033/0100 Train Loss:0.000089 \n",
      "Epoch/Iter:033/0200 Train Loss:0.000478 \n",
      "Epoch/Iter:033/0300 Train Loss:0.000088 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 034/050 68% [Remain: 0h/ 0m/40s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:034/0000 Train Loss:0.000020 \n",
      "Epoch/Iter:034/0100 Train Loss:0.000361 \n",
      "Epoch/Iter:034/0200 Train Loss:0.003105 \n",
      "Epoch/Iter:034/0300 Train Loss:0.000551 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 035/050 70% [Remain: 0h/ 0m/38s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:035/0000 Train Loss:0.000234 \n",
      "Epoch/Iter:035/0100 Train Loss:0.000461 \n",
      "Epoch/Iter:035/0200 Train Loss:0.001980 \n",
      "Epoch/Iter:035/0300 Train Loss:0.000163 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 036/050 72% [Remain: 0h/ 0m/35s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:036/0000 Train Loss:0.000355 \n",
      "Epoch/Iter:036/0100 Train Loss:0.000279 \n",
      "Epoch/Iter:036/0200 Train Loss:0.000126 \n",
      "Epoch/Iter:036/0300 Train Loss:0.000386 \n",
      "acc: 0.9984166666666666\n",
      "Epoch: 037/050 74% [Remain: 0h/ 0m/32s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:037/0000 Train Loss:0.001764 \n",
      "Epoch/Iter:037/0100 Train Loss:0.000276 \n",
      "Epoch/Iter:037/0200 Train Loss:0.000517 \n",
      "Epoch/Iter:037/0300 Train Loss:0.000428 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 038/050 76% [Remain: 0h/ 0m/30s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:038/0000 Train Loss:0.000052 \n",
      "Epoch/Iter:038/0100 Train Loss:0.005131 \n",
      "Epoch/Iter:038/0200 Train Loss:0.000249 \n",
      "Epoch/Iter:038/0300 Train Loss:0.000131 \n",
      "acc: 0.99875\n",
      "Epoch: 039/050 78% [Remain: 0h/ 0m/27s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:039/0000 Train Loss:0.000265 \n",
      "Epoch/Iter:039/0100 Train Loss:0.001273 \n",
      "Epoch/Iter:039/0200 Train Loss:0.000116 \n",
      "Epoch/Iter:039/0300 Train Loss:0.000366 \n",
      "acc: 0.9985833333333334\n",
      "Epoch: 040/050 80% [Remain: 0h/ 0m/25s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:040/0000 Train Loss:0.000120 \n",
      "Epoch/Iter:040/0100 Train Loss:0.000783 \n",
      "Epoch/Iter:040/0200 Train Loss:0.000430 \n",
      "Epoch/Iter:040/0300 Train Loss:0.000545 \n",
      "acc: 0.9988333333333334\n",
      "Epoch: 041/050 82% [Remain: 0h/ 0m/22s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:041/0000 Train Loss:0.000036 \n",
      "Epoch/Iter:041/0100 Train Loss:0.000738 \n",
      "Epoch/Iter:041/0200 Train Loss:0.000213 \n",
      "Epoch/Iter:041/0300 Train Loss:0.000122 \n",
      "acc: 0.9985833333333334\n",
      "Epoch: 042/050 84% [Remain: 0h/ 0m/20s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:042/0000 Train Loss:0.000168 \n",
      "Epoch/Iter:042/0100 Train Loss:0.000104 \n",
      "Epoch/Iter:042/0200 Train Loss:0.000172 \n",
      "Epoch/Iter:042/0300 Train Loss:0.000328 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 043/050 86% [Remain: 0h/ 0m/17s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:043/0000 Train Loss:0.000304 \n",
      "Epoch/Iter:043/0100 Train Loss:0.000096 \n",
      "Epoch/Iter:043/0200 Train Loss:0.000034 \n",
      "Epoch/Iter:043/0300 Train Loss:0.000098 \n",
      "acc: 0.9985\n",
      "Epoch: 044/050 88% [Remain: 0h/ 0m/15s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:044/0000 Train Loss:0.000042 \n",
      "Epoch/Iter:044/0100 Train Loss:0.000050 \n",
      "Epoch/Iter:044/0200 Train Loss:0.000307 \n",
      "Epoch/Iter:044/0300 Train Loss:0.001145 \n",
      "acc: 0.9985\n",
      "Epoch: 045/050 90% [Remain: 0h/ 0m/12s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:045/0000 Train Loss:0.000028 \n",
      "Epoch/Iter:045/0100 Train Loss:0.002094 \n",
      "Epoch/Iter:045/0200 Train Loss:0.001381 \n",
      "Epoch/Iter:045/0300 Train Loss:0.000188 \n",
      "acc: 0.9985\n",
      "Epoch: 046/050 92% [Remain: 0h/ 0m/10s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:046/0000 Train Loss:0.000092 \n",
      "Epoch/Iter:046/0100 Train Loss:0.002012 \n",
      "Epoch/Iter:046/0200 Train Loss:0.000074 \n",
      "Epoch/Iter:046/0300 Train Loss:0.001146 \n",
      "acc: 0.9985\n",
      "Epoch: 047/050 94% [Remain: 0h/ 0m/ 7s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:047/0000 Train Loss:0.000718 \n",
      "Epoch/Iter:047/0100 Train Loss:0.000054 \n",
      "Epoch/Iter:047/0200 Train Loss:0.000039 \n",
      "Epoch/Iter:047/0300 Train Loss:0.001124 \n",
      "acc: 0.99825\n",
      "Epoch: 048/050 96% [Remain: 0h/ 0m/ 5s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:048/0000 Train Loss:0.000449 \n",
      "Epoch/Iter:048/0100 Train Loss:0.001647 \n",
      "Epoch/Iter:048/0200 Train Loss:0.000218 \n",
      "Epoch/Iter:048/0300 Train Loss:0.000029 \n",
      "acc: 0.9985\n",
      "Epoch: 049/050 98% [Remain: 0h/ 0m/ 2s | Avg: 1.91s/Epoch]\n",
      "\n",
      "Epoch/Iter:049/0000 Train Loss:0.000876 \n",
      "Epoch/Iter:049/0100 Train Loss:0.000091 \n",
      "Epoch/Iter:049/0200 Train Loss:0.000633 \n",
      "Epoch/Iter:049/0300 Train Loss:0.008114 \n",
      "acc: 0.9986666666666667\n",
      "Epoch: 050/050 100% [Remain: 0h/ 0m/ 0s | Avg: 1.91s/Epoch]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import marveltoolbox as mt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LoRAConfs(mt.BaseConfs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        self.dataset = 'mnist'\n",
    "        self.nc = 1\n",
    "        self.nz = 10\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 50\n",
    "        self.seed = 0\n",
    "        self.r = 4\n",
    "        self.alpha = 1.0\n",
    "\n",
    "    def get_flag(self):\n",
    "        self.flag = 'demo-{}-clf-lora-r{}-alpha{}'.format(self.dataset, self.r, self.alpha)\n",
    "\n",
    "    def get_device(self):\n",
    "        self.device_ids = [0]\n",
    "        self.ngpu = len(self.device_ids)\n",
    "        self.device = torch.device(\n",
    "            \"cuda:{}\".format(self.device_ids[0]) if \\\n",
    "            (torch.cuda.is_available() and self.ngpu > 0) else \"mps\")\n",
    "\n",
    "\n",
    "class LoRATrainer(mt.BaseTrainer, LoRAConfs):\n",
    "    def __init__(self, BaseModel):\n",
    "        LoRAConfs.__init__(self)\n",
    "        mt.BaseTrainer.__init__(self, self)\n",
    "        \n",
    "        # To inject LoRA (Low-Rank Adaptation) into a regular ⁠nn.Module using the ⁠inject_lora function\n",
    "        self.models['C'] = mt.utils.inject_lora(BaseModel, r=self.r, alpha=self.alpha).to(self.device) \n",
    "\n",
    "        # Extract the trainable parameters of the LoRA module, then pass it to the optimizer.\n",
    "        self.optims['C'] = torch.optim.Adam(\n",
    "            mt.utils.get_lora_parameters(self.models['C']), lr=1e-4, betas=(0.5, 0.99))\n",
    "        \n",
    "        self.train_loader, self.val_loader, self.test_loader, _ = \\\n",
    "            mt.datasets.load_data(self.dataset, 1.0, 0.8, self.batch_size, 32, None, False)\n",
    "\n",
    "        self.records['acc'] = 0.0\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.models['C'].train()\n",
    "        for i, (x, y) in enumerate(self.train_loader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            scores = self.models['C'](x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            self.optims['C'].zero_grad()\n",
    "            loss.backward()\n",
    "            self.optims['C'].step()\n",
    "            if i % 100 == 0:\n",
    "                self.logs['Train Loss'] = loss.item()\n",
    "                self.print_logs(epoch, i)\n",
    "\n",
    "        return loss.item()\n",
    "                \n",
    "    def eval(self, epoch):\n",
    "        self.models['C'].eval()\n",
    "        correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                N = len(x)\n",
    "                scores = self.models['C'](x)\n",
    "                pred_y = torch.argmax(scores, dim=1)\n",
    "                correct += torch.sum(pred_y == y).item()\n",
    "        N = len(self.val_loader.dataset)\n",
    "        acc = correct / N\n",
    "        is_best = False\n",
    "        if acc >= self.records['acc']:\n",
    "            is_best = True\n",
    "            self.records['acc'] = acc\n",
    "        print('acc: {}'.format(acc))\n",
    "        return is_best\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer_lora = LoRATrainer(trainer_base.models['C']) # Feed the base model to LoRATrainer\n",
    "    trainer_lora.run(load_best=False, retrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1d55f",
   "metadata": {},
   "source": [
    "# Step3 Extract，Inject or Merge existing LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ded140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf\n",
      "Batch size: 128\n",
      "Epochs:     50\n",
      "device:     cuda:0\n",
      "\n",
      "image range [0, 1]\n",
      "Enet32(\n",
      "  (main): Sequential(\n",
      "    (0): NormalizedModel()\n",
      "    (1): LoRAConv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): LoRAConv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): LoRAConv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): LoRAConv2d(1024, 128, kernel_size=(4, 4), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): LoRAConv2d(128, 20, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load and extract existing LoRA\n",
    "lora = mt.utils.extract_lora_state_dict(trainer_lora.models['C']) \n",
    "\n",
    "# Inject LoRA into the new model.\n",
    "new_trainer_base = Trainer()\n",
    "new_trainer_base.models['C'] = mt.utils.inject_lora(new_trainer_base.models['C'], r=4, alpha=1, lora_state_dict=lora)\n",
    "print(new_trainer_base.models['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b29a26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enet32(\n",
      "  (main): Sequential(\n",
      "    (0): NormalizedModel()\n",
      "    (1): Conv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Conv2d(1024, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Merge LoRA to origial weight.\n",
    "new_trainer_base.models['C'] = mt.utils.merge_lora_weights(new_trainer_base.models['C'])\n",
    "print(new_trainer_base.models['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701370d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
