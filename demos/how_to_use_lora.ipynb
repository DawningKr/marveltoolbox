{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd247d4",
   "metadata": {},
   "source": [
    "# Step 1: Define trainer without LoRA to train the Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24462aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf\n",
      "Batch size: 128\n",
      "Epochs:     50\n",
      "device:     cuda:0\n",
      "\n",
      "image range [0, 1]\n",
      "Set random seed to: 0\n",
      "Log file save at:  ./logs/demo-mnist-clf.log\n",
      "=> loading checkpoint './chkpts/checkpoint_demo-mnist-clf.pth.tar'\n",
      "=> loaded checkpoint (epoch 50)\n"
     ]
    }
   ],
   "source": [
    "import marveltoolbox as mt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Confs(mt.BaseConfs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        self.dataset = 'mnist'\n",
    "        self.nc = 1\n",
    "        self.nz = 10\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 50\n",
    "        self.seed = 0\n",
    "\n",
    "    def get_flag(self):\n",
    "        self.flag = 'demo-{}-clf'.format(self.dataset)\n",
    "\n",
    "    def get_device(self):\n",
    "        self.device_ids = [0]\n",
    "        self.ngpu = len(self.device_ids)\n",
    "        self.device = torch.device(\n",
    "            \"cuda:{}\".format(self.device_ids[0]) if \\\n",
    "            (torch.cuda.is_available() and self.ngpu > 0) else \"mps\")\n",
    "\n",
    "\n",
    "class Trainer(mt.BaseTrainer, Confs):\n",
    "    def __init__(self):\n",
    "        Confs.__init__(self)\n",
    "        mt.BaseTrainer.__init__(self, self)\n",
    "        \n",
    "        self.models['C'] = mt.nn.dcgan.Enet32(self.nc, self.nz).to(self.device) \n",
    "\n",
    "        self.optims['C'] = torch.optim.Adam(\n",
    "            self.models['C'].parameters(), lr=1e-4, betas=(0.5, 0.99))\n",
    "        \n",
    "        self.train_loader, self.val_loader, self.test_loader, _ = \\\n",
    "            mt.datasets.load_data(self.dataset, 1.0, 0.8, self.batch_size, 32, None, False)\n",
    "\n",
    "        self.records['acc'] = 0.0\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.models['C'].train()\n",
    "        for i, (x, y) in enumerate(self.train_loader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            scores = self.models['C'](x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            self.optims['C'].zero_grad()\n",
    "            loss.backward()\n",
    "            self.optims['C'].step()\n",
    "            if i % 100 == 0:\n",
    "                self.logs['Train Loss'] = loss.item()\n",
    "                self.print_logs(epoch, i)\n",
    "\n",
    "        return loss.item()\n",
    "                \n",
    "    def eval(self, epoch):\n",
    "        self.models['C'].eval()\n",
    "        correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                N = len(x)\n",
    "                scores = self.models['C'](x)\n",
    "                pred_y = torch.argmax(scores, dim=1)\n",
    "                correct += torch.sum(pred_y == y).item()\n",
    "        N = len(self.val_loader.dataset)\n",
    "        acc = correct / N\n",
    "        is_best = False\n",
    "        if acc >= self.records['acc']:\n",
    "            is_best = True\n",
    "            self.records['acc'] = acc\n",
    "        print('acc: {}'.format(acc))\n",
    "        return is_best\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer_base = Trainer()\n",
    "    trainer_base.run(load_best=False, retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3180477",
   "metadata": {},
   "source": [
    "# Step2: Define a LoRA trainer that takes a base model as input and injects LoRA weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fd809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf-lora\n",
      "Batch size: 128\n",
      "Epochs:     50\n",
      "device:     cuda:0\n",
      "\n",
      "image range [0, 1]\n",
      "Set random seed to: 0\n",
      "Log file save at:  ./logs/demo-mnist-clf-lora.log\n",
      "=> loading checkpoint './chkpts/checkpoint_demo-mnist-clf-lora.pth.tar'\n",
      "=> loaded checkpoint (epoch 50)\n"
     ]
    }
   ],
   "source": [
    "import marveltoolbox as mt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LoRAConfs(mt.BaseConfs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        self.dataset = 'mnist'\n",
    "        self.nc = 1\n",
    "        self.nz = 10\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 50\n",
    "        self.seed = 0\n",
    "\n",
    "    def get_flag(self):\n",
    "        self.flag = 'demo-{}-clf-lora'.format(self.dataset)\n",
    "\n",
    "    def get_device(self):\n",
    "        self.device_ids = [0]\n",
    "        self.ngpu = len(self.device_ids)\n",
    "        self.device = torch.device(\n",
    "            \"cuda:{}\".format(self.device_ids[0]) if \\\n",
    "            (torch.cuda.is_available() and self.ngpu > 0) else \"mps\")\n",
    "\n",
    "\n",
    "class LoRATrainer(mt.BaseTrainer, LoRAConfs):\n",
    "    def __init__(self, BaseModel):\n",
    "        LoRAConfs.__init__(self)\n",
    "        mt.BaseTrainer.__init__(self, self)\n",
    "        \n",
    "        # To inject LoRA (Low-Rank Adaptation) into a regular ⁠nn.Module using the ⁠inject_lora function\n",
    "        self.models['C'] = mt.utils.inject_lora(BaseModel, r=2, alpha=1).to(self.device) \n",
    "\n",
    "        # Extract the trainable parameters of the LoRA module, then pass it to the optimizer.\n",
    "        self.optims['C'] = torch.optim.Adam(\n",
    "            mt.utils.get_lora_parameters(self.models['C']), lr=1e-4, betas=(0.5, 0.99))\n",
    "        \n",
    "        self.train_loader, self.val_loader, self.test_loader, _ = \\\n",
    "            mt.datasets.load_data(self.dataset, 1.0, 0.8, self.batch_size, 32, None, False)\n",
    "\n",
    "        self.records['acc'] = 0.0\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.models['C'].train()\n",
    "        for i, (x, y) in enumerate(self.train_loader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            scores = self.models['C'](x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            self.optims['C'].zero_grad()\n",
    "            loss.backward()\n",
    "            self.optims['C'].step()\n",
    "            if i % 100 == 0:\n",
    "                self.logs['Train Loss'] = loss.item()\n",
    "                self.print_logs(epoch, i)\n",
    "\n",
    "        return loss.item()\n",
    "                \n",
    "    def eval(self, epoch):\n",
    "        self.models['C'].eval()\n",
    "        correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                N = len(x)\n",
    "                scores = self.models['C'](x)\n",
    "                pred_y = torch.argmax(scores, dim=1)\n",
    "                correct += torch.sum(pred_y == y).item()\n",
    "        N = len(self.val_loader.dataset)\n",
    "        acc = correct / N\n",
    "        is_best = False\n",
    "        if acc >= self.records['acc']:\n",
    "            is_best = True\n",
    "            self.records['acc'] = acc\n",
    "        print('acc: {}'.format(acc))\n",
    "        return is_best\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer_lora = LoRATrainer(trainer_base.models['C']) # Feed the base model to LoRATrainer\n",
    "    trainer_lora.run(load_best=False, retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1d55f",
   "metadata": {},
   "source": [
    "# Step3 Extract，Inject or Merge existing LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ded140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf\n",
      "Batch size: 128\n",
      "Epochs:     50\n",
      "device:     cuda:0\n",
      "\n",
      "image range [0, 1]\n",
      "Enet32(\n",
      "  (main): Sequential(\n",
      "    (0): NormalizedModel()\n",
      "    (1): LoRAConv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=2, alpha=1)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): LoRAConv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=2, alpha=1)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): LoRAConv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=2, alpha=1)\n",
      "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): LoRAConv2d(1024, 128, kernel_size=(4, 4), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, r=2, alpha=1)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): LoRAConv2d(128, 20, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, r=2, alpha=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load and extract existing LoRA\n",
    "lora = mt.utils.extract_lora_state_dict(trainer_lora.models['C']) \n",
    "\n",
    "# Inject LoRA into the new model.\n",
    "new_trainer_base = Trainer()\n",
    "new_trainer_base.models['C'] = mt.utils.inject_lora(new_trainer_base.models['C'], r=2, alpha=1, lora_state_dict=lora)\n",
    "print(new_trainer_base.models['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b29a26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enet32(\n",
      "  (main): Sequential(\n",
      "    (0): NormalizedModel()\n",
      "    (1): Conv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Conv2d(1024, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Merge LoRA to origial weight.\n",
    "new_trainer_base.models['C'] = mt.utils.merge_lora_weights(new_trainer_base.models['C'])\n",
    "print(new_trainer_base.models['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701370d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
