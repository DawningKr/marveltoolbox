{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd247d4",
   "metadata": {},
   "source": [
    "# Step 1: Define trainer without LoRA to train the Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24462aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf\n",
      "Batch size: 128\n",
      "Epochs:     2\n",
      "device:     mps\n",
      "\n",
      "image range [0, 1]\n",
      "Set random seed to: 0\n",
      "Log file save at:  ./logs/demo-mnist-clf.log\n",
      "Epoch/Iter:000/0000 Train Loss:3.011259 \n",
      "Epoch/Iter:000/0100 Train Loss:0.457619 \n",
      "Epoch/Iter:000/0200 Train Loss:0.213737 \n",
      "Epoch/Iter:000/0300 Train Loss:0.171058 \n",
      "acc: 0.9818333333333333\n",
      "Epoch: 001/002 50% [Remain: 0h/ 0m/26s | Avg: 26.30s/Epoch | {load: 18.10s | train: 2.78s | fp: 0.70s | bp: 0.60s | op: 1.48s | eval: 3.12s |}/Epoch]\n",
      "\n",
      "Epoch/Iter:001/0000 Train Loss:0.192348 \n",
      "Epoch/Iter:001/0100 Train Loss:0.113833 \n",
      "Epoch/Iter:001/0200 Train Loss:0.067389 \n",
      "Epoch/Iter:001/0300 Train Loss:0.101932 \n",
      "acc: 0.9873333333333333\n",
      "Epoch: 002/002 100% [Remain: 0h/ 0m/ 0s | Avg: 26.63s/Epoch | {load: 18.50s | train: 2.66s | fp: 0.65s | bp: 0.58s | op: 1.43s | eval: 3.14s |}/Epoch]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import marveltoolbox as mt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Confs(mt.BaseConfs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        self.dataset = 'mnist'\n",
    "        self.nc = 1\n",
    "        self.nz = 10\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 2\n",
    "        self.seed = 0\n",
    "\n",
    "    def get_flag(self):\n",
    "        self.flag = 'demo-{}-clf'.format(self.dataset)\n",
    "\n",
    "    def get_device(self):\n",
    "        self.device_ids = [0]\n",
    "        self.ngpu = len(self.device_ids)\n",
    "        self.device = torch.device(\n",
    "            \"cuda:{}\".format(self.device_ids[0]) if \\\n",
    "            (torch.cuda.is_available() and self.ngpu > 0) else \"mps\")\n",
    "\n",
    "\n",
    "class Trainer(mt.BaseTrainer, Confs):\n",
    "    def __init__(self):\n",
    "        Confs.__init__(self)\n",
    "        mt.BaseTrainer.__init__(self, self)\n",
    "        \n",
    "        self.models['C'] = mt.nn.dcgan.Enet32(self.nc, self.nz).to(self.device) \n",
    "\n",
    "        self.optims['C'] = torch.optim.Adam(\n",
    "            self.models['C'].parameters(), lr=1e-4, betas=(0.5, 0.99))\n",
    "        \n",
    "        self.train_loader, self.val_loader, self.test_loader, _ = \\\n",
    "            mt.datasets.load_data(self.dataset, 1.0, 0.8, self.batch_size, 32, None, False, num_workers=4)\n",
    "\n",
    "        self.records['acc'] = 0.0\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.models['C'].train()\n",
    "        for i, (x, y) in enumerate(self.train_loader):\n",
    "            self.timer.eval_begin('load')\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.timer.eval_end('load')\n",
    "            self.timer.eval_begin('train')\n",
    "            self.timer.eval_begin('fp')\n",
    "            scores = self.models['C'](x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            self.timer.eval_end('fp')\n",
    "            self.timer.eval_begin('bp')\n",
    "            self.optims['C'].zero_grad()\n",
    "            loss.backward()\n",
    "            self.timer.eval_end('bp')\n",
    "            self.timer.eval_begin('op')\n",
    "            self.optims['C'].step()\n",
    "            self.timer.eval_end('op')\n",
    "            self.timer.eval_end('train')\n",
    "            if i % 100 == 0:\n",
    "                self.logs['Train Loss'] = loss.item()\n",
    "                self.print_logs(epoch, i)\n",
    "\n",
    "\n",
    "        return loss.item()\n",
    "                \n",
    "    def eval(self, epoch):\n",
    "        self.timer.eval_begin('eval')\n",
    "        self.models['C'].eval()\n",
    "        correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                N = len(x)\n",
    "                scores = self.models['C'](x)\n",
    "                pred_y = torch.argmax(scores, dim=1)\n",
    "                correct += torch.sum(pred_y == y).item()\n",
    "        N = len(self.val_loader.dataset)\n",
    "        acc = correct / N\n",
    "        is_best = False\n",
    "        if acc >= self.records['acc']:\n",
    "            is_best = True\n",
    "            self.records['acc'] = acc\n",
    "        print('acc: {}'.format(acc))\n",
    "        self.timer.eval_end('eval')\n",
    "        return is_best\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer_base = Trainer()\n",
    "    trainer_base.run(load_best=False, retrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3180477",
   "metadata": {},
   "source": [
    "# Step2: Define a LoRA trainer that takes a base model as input and injects LoRA weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fd809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf-lora-r4-alpha1.0\n",
      "Batch size: 128\n",
      "Epochs:     2\n",
      "device:     mps\n",
      "\n",
      "12717734\n",
      "126352\n",
      "image range [0, 1]\n",
      "Set random seed to: 0\n",
      "Log file save at:  ./logs/demo-mnist-clf-lora-r4-alpha1.0.log\n",
      "Epoch/Iter:000/0000 Train Loss:0.056534 \n",
      "Epoch/Iter:000/0100 Train Loss:0.111727 \n",
      "Epoch/Iter:000/0200 Train Loss:0.021906 \n",
      "Epoch/Iter:000/0300 Train Loss:0.024588 \n",
      "acc: 0.9928333333333333\n",
      "Epoch: 001/002 50% [Remain: 0h/ 0m/25s | Avg: 25.32s/Epoch | {load: 16.85s | train: 3.07s | fp: 0.88s | bp: 0.71s | op: 1.48s | eval: 3.18s |}/Epoch]\n",
      "\n",
      "Epoch/Iter:001/0000 Train Loss:0.037745 \n",
      "Epoch/Iter:001/0100 Train Loss:0.035995 \n",
      "Epoch/Iter:001/0200 Train Loss:0.014037 \n",
      "Epoch/Iter:001/0300 Train Loss:0.019534 \n",
      "acc: 0.9941666666666666\n",
      "Epoch: 002/002 100% [Remain: 0h/ 0m/ 0s | Avg: 26.56s/Epoch | {load: 17.86s | train: 3.09s | fp: 0.90s | bp: 0.72s | op: 1.47s | eval: 3.33s |}/Epoch]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import marveltoolbox as mt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LoRAConfs(mt.BaseConfs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        self.dataset = 'mnist'\n",
    "        self.nc = 1\n",
    "        self.nz = 10\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 2\n",
    "        self.seed = 0\n",
    "        self.r = 4\n",
    "        self.alpha = 1.0\n",
    "\n",
    "    def get_flag(self):\n",
    "        self.flag = 'demo-{}-clf-lora-r{}-alpha{}'.format(self.dataset, self.r, self.alpha)\n",
    "\n",
    "    def get_device(self):\n",
    "        self.device_ids = [0]\n",
    "        self.ngpu = len(self.device_ids)\n",
    "        self.device = torch.device(\n",
    "            \"cuda:{}\".format(self.device_ids[0]) if \\\n",
    "            (torch.cuda.is_available() and self.ngpu > 0) else \"mps\")\n",
    "\n",
    "\n",
    "class LoRATrainer(mt.BaseTrainer, LoRAConfs):\n",
    "    def __init__(self, BaseModel):\n",
    "        LoRAConfs.__init__(self)\n",
    "        mt.BaseTrainer.__init__(self, self)\n",
    "        \n",
    "        # To inject LoRA (Low-Rank Adaptation) into a regular ⁠nn.Module using the ⁠inject_lora function\n",
    "        self.models['C'] = mt.utils.inject_lora(BaseModel, r=self.r, alpha=self.alpha).to(self.device) \n",
    "\n",
    "        # Extract the trainable parameters of the LoRA module, then pass it to the optimizer.\n",
    "        lora_params = mt.utils.get_lora_parameters(self.models['C'])\n",
    "        print(sum(p.numel() for p in self.models['C'].parameters()))\n",
    "        print(sum(p.numel() for p in lora_params))\n",
    "        self.optims['C'] = torch.optim.Adam(\n",
    "            lora_params, lr=1e-4, betas=(0.5, 0.99))\n",
    "        \n",
    "        self.train_loader, self.val_loader, self.test_loader, _ = \\\n",
    "            mt.datasets.load_data(self.dataset, 1.0, 0.8, self.batch_size, 32, None, False)\n",
    "\n",
    "        self.records['acc'] = 0.0\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.models['C'].train()\n",
    "        for i, (x, y) in enumerate(self.train_loader):\n",
    "            self.timer.eval_begin('load')\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.timer.eval_end('load')\n",
    "            self.timer.eval_begin('train')\n",
    "            self.timer.eval_begin('fp')\n",
    "            scores = self.models['C'](x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            self.timer.eval_end('fp')\n",
    "            self.timer.eval_begin('bp')\n",
    "            self.optims['C'].zero_grad()\n",
    "            loss.backward()\n",
    "            self.timer.eval_end('bp')\n",
    "            self.timer.eval_begin('op')\n",
    "            self.optims['C'].step()\n",
    "            self.timer.eval_end('op')\n",
    "            self.timer.eval_end('train')\n",
    "            if i % 100 == 0:\n",
    "                self.logs['Train Loss'] = loss.item()\n",
    "                self.print_logs(epoch, i)\n",
    "\n",
    "\n",
    "        return loss.item()\n",
    "                \n",
    "    def eval(self, epoch):\n",
    "        self.timer.eval_begin('eval')\n",
    "        self.models['C'].eval()\n",
    "        correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                N = len(x)\n",
    "                scores = self.models['C'](x)\n",
    "                pred_y = torch.argmax(scores, dim=1)\n",
    "                correct += torch.sum(pred_y == y).item()\n",
    "        N = len(self.val_loader.dataset)\n",
    "        acc = correct / N\n",
    "        is_best = False\n",
    "        if acc >= self.records['acc']:\n",
    "            is_best = True\n",
    "            self.records['acc'] = acc\n",
    "        print('acc: {}'.format(acc))\n",
    "        self.timer.eval_end('eval')\n",
    "        return is_best\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer_lora = LoRATrainer(trainer_base.models['C']) # Feed the base model to LoRATrainer\n",
    "    trainer_lora.run(load_best=False, retrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1d55f",
   "metadata": {},
   "source": [
    "# Step3 Extract，Inject or Merge existing LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ded140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "Flag:       demo-mnist-clf\n",
      "Batch size: 128\n",
      "Epochs:     50\n",
      "device:     cuda:0\n",
      "\n",
      "image range [0, 1]\n",
      "Enet32(\n",
      "  (main): Sequential(\n",
      "    (0): NormalizedModel()\n",
      "    (1): LoRAConv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): LoRAConv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): LoRAConv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): LoRAConv2d(1024, 128, kernel_size=(4, 4), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): LoRAConv2d(128, 20, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, r=4, alpha=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load and extract existing LoRA\n",
    "lora = mt.utils.extract_lora_state_dict(trainer_lora.models['C']) \n",
    "\n",
    "# Inject LoRA into the new model.\n",
    "new_trainer_base = Trainer()\n",
    "new_trainer_base.models['C'] = mt.utils.inject_lora(new_trainer_base.models['C'], r=4, alpha=1, lora_state_dict=lora)\n",
    "print(new_trainer_base.models['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b29a26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enet32(\n",
      "  (main): Sequential(\n",
      "    (0): NormalizedModel()\n",
      "    (1): Conv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Conv2d(1024, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Merge LoRA to origial weight.\n",
    "new_trainer_base.models['C'] = mt.utils.merge_lora_weights(new_trainer_base.models['C'])\n",
    "print(new_trainer_base.models['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701370d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
